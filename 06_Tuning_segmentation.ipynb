{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-christmas",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from pathlib import Path\n",
    "from timm import create_model\n",
    "from fastai.callback.fp16 import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-professional",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=2021\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    #tf.set_random_seed(seed)\n",
    "seed_everything(SEED)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-action",
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_PATH = \"./data\"\n",
    "path = Path(YOUR_PATH)\n",
    "\n",
    "codes = [\"Background\", \"Divots\", \"Cracks\", \"Scratches\", \"Ablation\"]\n",
    "\n",
    "def get_mask(fn):\n",
    "    fn = Path(str(fn).replace('train_images', 'masks').replace('jpg','png'))\n",
    "    return PILMask.create(fn)\n",
    "\n",
    "def get_grayscale(fn):\n",
    "    return np.array(Image.open(fn))[...,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-category",
   "metadata": {},
   "outputs": [],
   "source": [
    "severstal_stats=([0.343], [0.197])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-anatomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_with_defects(path):\n",
    "    L = get_image_files(path)\n",
    "    L = [p for p in L if np.max(np.array(get_mask(p))) > 0]\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-kinase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dls_gray(size, batch_size=8):\n",
    "  \n",
    "  dblock = DataBlock(blocks=(ImageBlock, MaskBlock(codes)),\n",
    "                     get_items=get_image_files,\n",
    "                     get_x=get_grayscale,\n",
    "                     get_y = get_mask,\n",
    "                     batch_tfms=[*aug_transforms(size=size,flip_vert=True), Normalize.from_stats(*severstal_stats)])\n",
    "  \n",
    "  return dblock.dataloaders(path/'train_images', batch_size=batch_size)\n",
    "\n",
    "def get_dls_rgb(size, batch_size=8):\n",
    "  \n",
    "  dblock = DataBlock(blocks=(ImageBlock, MaskBlock(codes)),\n",
    "                     get_items=get_images_with_defects,\n",
    "                     get_y = get_mask,\n",
    "                     batch_tfms=[*aug_transforms(size=size,flip_vert=True), Normalize.from_stats(*severstal_stats)])\n",
    "  \n",
    "  return dblock.dataloaders(path/'train_images', batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-gather",
   "metadata": {},
   "outputs": [],
   "source": [
    "def StratMaskSplitter(test_size=0.2, random_state=None, train_size=None, shuffle=True):\n",
    "    \"Split `items` into random train and test stratifying by smallest defect in the mask\"\n",
    "    \n",
    "    def get_smallest_label(mask):\n",
    "        vals, counts = tensor(mask).unique(return_counts=True)\n",
    "        return vals[torch.argmin(counts)]\n",
    "    \n",
    "    def _inner(o, **kwargs):\n",
    "        \n",
    "        labels = tensor([get_smallest_label(get_mask(fn)) for fn in o])       \n",
    "        \n",
    "        train,valid = train_test_split(range_of(o), test_size=test_size, random_state=random_state,\n",
    "                                        stratify=labels, train_size=train_size, shuffle=shuffle)\n",
    "        return L(train), L(valid)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-individual",
   "metadata": {},
   "outputs": [],
   "source": [
    "dblock = DataBlock(blocks=(ImageBlock, MaskBlock(codes)),\n",
    "                     get_items=get_images_with_defects,\n",
    "                     get_y = get_mask,\n",
    "                     splitter = StratMaskSplitter(),\n",
    "                     batch_tfms=[*aug_transforms(size=(256,512),flip_vert=True), Normalize.from_stats(*severstal_stats)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-kelly",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_strat = dblock.dataloaders(path/'train_images', batch_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-machine",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_strat.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-enclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = (256, 512) # As per 1st place solution\n",
    "bs = 6       # 1st place solution proposes 12 or 24 with grad accumulation at 24 samples\n",
    "\n",
    "dls_gray = get_dls_gray(size=sz, batch_size=bs)\n",
    "dls_rgb = get_dls_rgb(size=sz, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-assault",
   "metadata": {},
   "source": [
    "### Hacky TIMM encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-savannah",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_first_layer(model, n_in, pretrained):\n",
    "    \"Change first layer based on number of input channels\"\n",
    "    if n_in == 3: return\n",
    "    first_layer, parent, name = _get_first_layer(model)\n",
    "    assert isinstance(first_layer, nn.Conv2d), f'Change of input channels only supported with Conv2d, found {first_layer.__class__.__name__}'\n",
    "    assert getattr(first_layer, 'in_channels') == 3, f'Unexpected number of input channels, found {getattr(first_layer, \"in_channels\")} while expecting 3'\n",
    "    params = {attr:getattr(first_layer, attr) for attr in 'out_channels kernel_size stride padding dilation groups padding_mode'.split()}\n",
    "    params['bias'] = getattr(first_layer, 'bias') is not None\n",
    "    params['in_channels'] = n_in\n",
    "    new_layer = nn.Conv2d(**params)\n",
    "    if pretrained:\n",
    "        _load_pretrained_weights(new_layer, first_layer)\n",
    "    setattr(parent, name, new_layer)\n",
    "    \n",
    "def _add_norm(dls, meta, pretrained):\n",
    "    if not pretrained: return\n",
    "    stats = meta.get('stats')\n",
    "    if stats is None: return\n",
    "    if not dls.after_batch.fs.filter(risinstance(Normalize)):\n",
    "        dls.add_tfms([Normalize.from_stats(*stats)],'after_batch')\n",
    "    \n",
    "def create_timm_body(arch:str, pretrained=True, cut=None, n_in=3):\n",
    "    \"Creates a body from any model in the `timm` library.\"\n",
    "    model = create_model(arch, pretrained=pretrained, num_classes=0, global_pool='')\n",
    "    _update_first_layer(model, n_in, pretrained)\n",
    "    if cut is None:\n",
    "        ll = list(enumerate(model.children()))\n",
    "        cut = next(i for i,o in reversed(ll) if has_pool_type(o))\n",
    "    if isinstance(cut, int): return nn.Sequential(*list(model.children())[:cut])\n",
    "    elif callable(cut): return cut(model)\n",
    "    else: raise NamedError(\"cut must be either integer or function\")\n",
    "\n",
    "def unet_learner_timm(dls, arch, normalize=True, n_out=None, n_in=3, pretrained=True, config=None,\n",
    "                 # learner args\n",
    "                 loss_func=None, opt_func=Adam, lr=defaults.lr, splitter=None, cbs=None, metrics=None, path=None,\n",
    "                 model_dir='models', wd=None, wd_bn_bias=False, train_bn=True, moms=(0.95,0.85,0.95),\n",
    "                 # other model args\n",
    "                 **kwargs):\n",
    "    \"Build a unet learner from `dls` and `arch`\"\n",
    "\n",
    "    if config:\n",
    "        warnings.warn('config param is deprecated. Pass your args directly to unet_learner.')\n",
    "        kwargs = {**config, **kwargs}\n",
    "    _default_meta    = {'cut':None, 'split':default_split}\n",
    "    \n",
    "    \n",
    "    meta = model_meta.get(arch, _default_meta)\n",
    "    if normalize: _add_norm(dls, meta, pretrained)\n",
    "\n",
    "    n_out = ifnone(n_out, get_c(dls))\n",
    "    assert n_out, \"`n_out` is not defined, and could not be inferred from data, set `dls.c` or pass `n_out`\"\n",
    "    img_size = dls.one_batch()[0].shape[-2:]\n",
    "    assert img_size, \"image size could not be inferred from data\"\n",
    "    timm_body = create_timm_body(arch, n_in=n_in)\n",
    "    model = DynamicUnet(timm_body, n_out, img_size, **kwargs)\n",
    "\n",
    "    splitter=ifnone(splitter, meta['split'])\n",
    "    learn = Learner(dls=dls, model=model, loss_func=loss_func, opt_func=opt_func, lr=lr, splitter=splitter, cbs=cbs,\n",
    "                   metrics=metrics, path=path, model_dir=model_dir, wd=wd, wd_bn_bias=wd_bn_bias, train_bn=train_bn,\n",
    "                   moms=moms)\n",
    "    if pretrained: learn.freeze()\n",
    "    # keep track of args for loggers\n",
    "    store_attr('arch,normalize,n_out,pretrained', self=learn, **kwargs)\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-chapter",
   "metadata": {},
   "source": [
    "### Loss func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-young",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss:\n",
    "    \"Dice loss for segmentation\"\n",
    "    def __init__(self, axis=1, smooth=1):\n",
    "        store_attr()\n",
    "    def __call__(self, pred, targ):\n",
    "        targ = self._one_hot(targ, pred.shape[self.axis])\n",
    "        pred, targ = flatten_check(self.activation(pred), targ)\n",
    "        inter = (pred*targ).sum()\n",
    "        union = (pred+targ).sum()\n",
    "        return 1 - (2. * inter + self.smooth)/(union + self.smooth)\n",
    "    @staticmethod\n",
    "    def _one_hot(x, classes, axis=1):\n",
    "        \"Creates one binay mask per class\"\n",
    "        return torch.stack([torch.where(x==c, 1, 0) for c in range(classes)], axis=axis)\n",
    "    def activation(self, x): return F.softmax(x, dim=self.axis)\n",
    "    def decodes(self, x):    return x.argmax(dim=self.axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-japan",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomComboLoss:\n",
    "    \"Dice and Focal combined\"\n",
    "    def __init__(self, axis=1, smooth=1., alpha=0.75):\n",
    "        store_attr()\n",
    "        self.focal_loss = FocalLossFlat(axis=axis)\n",
    "        self.dice_loss =  DiceLoss(axis, smooth)       \n",
    "    def __call__(self, pred, targ):\n",
    "        return (self.alpha * self.focal_loss(pred, targ)) + ((1-self.alpha) * self.dice_loss(pred, targ))\n",
    "    def decodes(self, x):    return x.argmax(dim=self.axis)\n",
    "    def activation(self, x): return F.softmax(x, dim=self.axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-freeze",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-sequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learn = unet_learner(dls_strat, resnet50, loss_func=CustomComboLoss(alpha=0.5), metrics=[DiceMulti], opt_func=ranger, act_cls=Mish, cbs=[GradientAccumulation(n_acc=24)],self_attention = True).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-patrol",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-comparative",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_flat_cos(10, slice(1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-cruise",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-discrimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "lrs = slice(lr/100, lr)\n",
    "learn.fit_flat_cos(10, lrs, cbs = [GradientAccumulation(n_acc=24),ReduceLROnPlateau(factor=4,patience=2, min_lr=1e-9)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-grenada",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(\"UNET_R50_29.07.2021.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.loss_func = CustomComboLoss2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomComboLoss2:\n",
    "    \"Dice and Focal combined\"\n",
    "    def __init__(self, axis=1, smooth=1., alpha=0.8):\n",
    "        store_attr()\n",
    "        self.ce_loss = CrossEntropyLossFlat(axis=axis, weight=tensor(0.5,2.0,2.0,1.0,1.5))\n",
    "        self.dice_loss =  DiceLoss(axis, smooth)       \n",
    "    def __call__(self, pred, targ):\n",
    "        return (self.alpha * self.ce_loss(pred, targ)) + ((1-self.alpha) * self.dice_loss(pred, targ))\n",
    "    def decodes(self, x):    return x.argmax(dim=self.axis)\n",
    "    def activation(self, x): return F.softmax(x, dim=self.axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-parameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.opt.hypers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find(start_lr=1e-10, end_lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-flesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find() #old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-culture",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_new = learn = unet_learner(dls_strat, resnet50, metrics=[DiceMulti], opt_func=ranger, act_cls=Mish, cbs=[GradientAccumulation(n_acc=24)],self_attention = True).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-monaco",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_new.loss_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-underwear",
   "metadata": {},
   "source": [
    "### Adam16 optimizer to help with NaN (not needed, Ranger works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-screen",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "# This version of Adam keeps an fp32 copy of the parameters and \n",
    "# does all of the parameter updates in fp32, while still doing the\n",
    "# forwards and backwards passes using fp16 (i.e. fp16 copies of the \n",
    "# parameters and fp16 activations).\n",
    "#\n",
    "# Note that this calls .float().cuda() on the params such that it \n",
    "# moves them to gpu 0--if you're using a different GPU or want to \n",
    "# do multi-GPU you may need to deal with this.\n",
    "class Adam16(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay)\n",
    "        params = list(params)\n",
    "        super(Adam16, self).__init__(params, defaults)\n",
    "        # for group in self.param_groups:\n",
    "            # for p in group['params']:\n",
    "        \n",
    "        self.fp32_param_groups = [p.data.float().cuda() for p in params]\n",
    "        if not isinstance(self.fp32_param_groups[0], dict):\n",
    "            self.fp32_param_groups = [{'params': self.fp32_param_groups}]\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group,fp32_group in zip(self.param_groups,self.fp32_param_groups):\n",
    "            for p,fp32_p in zip(group['params'],fp32_group['params']):\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                    \n",
    "                grad = p.grad.data.float()\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = grad.new().resize_as_(grad).zero_()\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = grad.new().resize_as_(grad).zero_()\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad = grad.add(group['weight_decay'], fp32_p)\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "\n",
    "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
    "            \n",
    "                # print(type(fp32_p))\n",
    "                fp32_p.addcdiv_(-step_size, exp_avg, denom)\n",
    "                p.data = fp32_p.half()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-peter",
   "metadata": {},
   "source": [
    "### Jarvis.ai TIMM encoder implementation (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-weight",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Module):\n",
    "    def __init__(self, model_name='efficientnet_b0'):\n",
    "        self.encoder = create_model(model_name, features_only=True, pretrained=True)\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        return self.encoder.blocks[i]\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "def UnetBlock(Module):\n",
    "    def __init__(self, in_channels, channels, out_channels, act=ReLU, attn=None):\n",
    "        self.act = act\n",
    "        self.conv1(ConvLayer(in_channels,channels, act_cls = self.act))\n",
    "        self.conv2(ConvLayer(channels,out_channels, act_cls = self.act))\n",
    "        self.attn_layer = attn(out_channels) if attn else noop\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        x=self.conv1(x)\n",
    "        x=self.conv2(x)\n",
    "        x=self.attn_layer(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "def UnetDecoder(Module):\n",
    "    def __init__(self, fs=32, expansion = 4, n_out=1, hypercol=False, attn=None, act = ReLU):\n",
    "        center_ch = 512*expansion\n",
    "        decoder5_ch = center_ch + (256 * expansion)\n",
    "        channels=512\n",
    "        self.hypercol = hypercol\n",
    "        self.center = nn.Sequential(ConvLayer(center_ch,center_ch, act_cls=act), ConvLayer(center_ch,center_ch//2,act_cls=act))\n",
    "        self.decoder5 = UnetBlock(decoder5_ch, channels, fs, act, attn)\n",
    "        self.decoder4 = UnetBlock(256*expansion+fs, 256, fs, act, attn)\n",
    "        self.decoder3 = UnetBlock(128*expansion+fs, 128, fs, act, attn)\n",
    "        self.decoder2 = UnetBlock(64*expansion+fs, 64, fs, act, attn)\n",
    "        self.decoder1 = UnetBlock(fs, fs, fs, act, attn)\n",
    "        if hypercol:\n",
    "            self.logit = nn.Sequential(ConvLayer(fs*5,fs*2), ConvLayer(fs*2,fs),nn.Conv2d(fs,n_out,kernel_size=1))\n",
    "        else:\n",
    "            self.logit = nn.Sequential(ConvLayer(fs,fs//2), ConvLayer(fs//2,fs//2),nn.Conv2d(fs//2,n_out,kernel_size=1))\n",
    "    def forward(self, features):\n",
    "        e1,e2,e3,e4,e5 = features\n",
    "        f = self.center(e5)\n",
    "        d5 = self.decoder5(torch.cat([f,e5],1))\n",
    "        d4 = self.decoder4(torch.cat([d5,e4],1))\n",
    "        d3 = self.decoder3(torch.cat([d4,e3],1))\n",
    "        d2 = self.decoder2(torch.cat([d3,e2],1))\n",
    "        d1 = self.decoder1(d2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
